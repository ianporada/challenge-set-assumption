{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_names = [\n",
    "    'conll2012_indiscrim_english_v4',\n",
    "    'gum_indiscrim_ontogum',\n",
    "    'arrau_indiscrim_default',\n",
    "    'gap_indiscrim_default',\n",
    "    'davis_pdp_indiscrim_default',\n",
    "    'preco_indiscrim_default',\n",
    "    'litbank_indiscrim_split_0',\n",
    "    'gum_indiscrim_original',\n",
    "    'phrase_detectives_indiscrim_default',\n",
    "    'mmc_indiscrim_mmc_en',\n",
    "    'davis_wsc_indiscrim_wsc273',\n",
    "    'superglue_wsc_indiscrim_default',\n",
    "    'dpr_indiscrim_default',\n",
    "    'knowref_60k_indiscrim_default',\n",
    "    'pronominal_winogrande_default'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_mapping(sentences, context_start, context_end):\n",
    "    local_to_global = {}\n",
    "    global_to_local = {}\n",
    "    t = 0\n",
    "    for s_i in range(context_start, context_end):\n",
    "        sentence = sentences[s_i]\n",
    "        for i in range(len(sentence[\"tokens\"])):\n",
    "            local_to_global[(s_i, i)] = t\n",
    "            global_to_local[t] = (s_i, i)\n",
    "            t += 1\n",
    "    return local_to_global, global_to_local\n",
    "\n",
    "\n",
    "def local_mention_to_global(local_to_global, mention):\n",
    "    sent, start, end = mention\n",
    "    return (\n",
    "                local_to_global[(sent, start)],\n",
    "                local_to_global[(sent, end)]\n",
    "            )\n",
    "\n",
    "\n",
    "def global_mention_to_local(global_to_local, mention):\n",
    "    start, end = mention\n",
    "    start_sent, start_tok = global_to_local[start]\n",
    "    end_sent, end_tok = global_to_local[end]\n",
    "    assert start_sent == end_sent and end_tok >= start_tok\n",
    "    return [start_sent, start_tok, end_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(config_name, split, dataset, use_local_context, include_speaker):\n",
    "    examples = []\n",
    "    for ex in tqdm(dataset):\n",
    "        sentences = ex[\"sentences\"]\n",
    "\n",
    "        context_start = 0\n",
    "        context_end = len(sentences)\n",
    "\n",
    "        ex_id = ex[\"id\"]\n",
    "        psent, pstart, pend = ex[\"pronoun\"]\n",
    "        ex_id = str(ex[\"id\"]) + f\"_{psent}_{pstart}_{pend}\"\n",
    "        \n",
    "        if use_local_context:\n",
    "            context_start = ex[\"local_context_start\"]\n",
    "            context_end = ex[\"local_context_end\"]\n",
    "\n",
    "        local_to_global, global_to_local = get_token_mapping(sentences, context_start, context_end)\n",
    "        words = [[x[\"text\"] for x in s[\"tokens\"]] for s in sentences[context_start:context_end]]\n",
    "\n",
    "        speakers = None\n",
    "        if include_speaker:\n",
    "            speakers = [[s[\"speaker\"] if s[\"speaker\"] is not None else \"\"]*len(s[\"tokens\"])\n",
    "                        for s in sentences[context_start:context_end]]\n",
    "            speakers = [spk for s in speakers for spk in s]\n",
    "\n",
    "        lm_to_global = partial(local_mention_to_global, local_to_global)\n",
    "        mentions = [lm_to_global(ex[\"pronoun\"]),\n",
    "                    lm_to_global(ex[\"antecedents\"][0]),\n",
    "                    lm_to_global(ex[\"distractors\"][0])] # (start, end)\n",
    "        \n",
    "        # make sure each\n",
    "        instructions = \"Final Exam with Answer Key\\n\" \\\n",
    "                 \"Instructions: Please carefully read the following passages. \" \\\n",
    "                 \"For each passage, you must identify which noun the pronoun marked in *bold* refers to.\\n\" \\\n",
    "                 \"=====\\n\"\n",
    "        \n",
    "        passage_words = [w for s in words for w in s]\n",
    "        global_pronoun = mentions[0]\n",
    "        assert global_pronoun[0] == global_pronoun[1], \"Pronoun should be exactly one word\"\n",
    "        original_pronoun = passage_words[global_pronoun[0]]\n",
    "        passage_words[global_pronoun[0]] = f\"*{original_pronoun}*\" # add astericks around pronoun\n",
    "\n",
    "        if include_speaker:\n",
    "            last_speaker = None\n",
    "            passage = \"\"\n",
    "            for i, w in enumerate(passage_words):\n",
    "                curr_speaker = speakers[i] if speakers[i] else \"Anonymous\"\n",
    "                if curr_speaker != last_speaker:\n",
    "                    passage += f\"\\n\\n{curr_speaker}:\\n\"\n",
    "                    last_speaker = curr_speaker\n",
    "                passage += (\" \" if passage else \"\") + w\n",
    "        else:\n",
    "            passage = \" \".join(passage_words)\n",
    "\n",
    "        question = f\"In the passage above, what does the pronoun \\\"*{original_pronoun}*\\\" refer to?\"\n",
    "\n",
    "        input_str = instructions + \"Passage: \" + passage + \"\\n\" + \"Question: \" + question + \"\\n\" + \"Answer: \"\n",
    "\n",
    "        global_antecedent = mentions[1]\n",
    "        expected_output_words = passage_words[global_antecedent[0] : global_antecedent[1] + 1]\n",
    "        expected_output = \" \".join(expected_output_words).lower()\n",
    "\n",
    "        distractor_antecedent = mentions[2]\n",
    "        negative_output = passage_words[distractor_antecedent[0] : distractor_antecedent[1] + 1]\n",
    "        negative_output = \" \".join(negative_output).lower()\n",
    "        \n",
    "        # (dataset, split, example_id, local_context, include_speaker, input, expected_output)\n",
    "        output_example = {\n",
    "            \"dataset\": config_name,\n",
    "            \"split\": split,\n",
    "            \"example_id\": ex_id,\n",
    "            \"local_context\": use_local_context,\n",
    "            \"include_speaker\": include_speaker,\n",
    "            \"input\": input_str,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"negative_output\": negative_output,\n",
    "            \"passage_words\": passage_words,\n",
    "            \"mentions\": mentions,\n",
    "        }\n",
    "        examples.append(output_example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [00:06<00:00, 219.88it/s]\n",
      "100%|██████████| 1536/1536 [00:06<00:00, 246.37it/s]\n",
      "100%|██████████| 1536/1536 [00:06<00:00, 222.67it/s]\n",
      "100%|██████████| 1536/1536 [00:11<00:00, 139.29it/s]\n",
      "100%|██████████| 1642/1642 [00:08<00:00, 200.14it/s]\n",
      "100%|██████████| 1642/1642 [00:07<00:00, 233.72it/s]\n",
      "100%|██████████| 1642/1642 [00:08<00:00, 195.82it/s]\n",
      "100%|██████████| 1642/1642 [00:07<00:00, 213.98it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 98.60it/s] \n",
      "100%|██████████| 272/272 [00:02<00:00, 113.68it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 106.38it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 108.40it/s]\n",
      "100%|██████████| 236/236 [00:02<00:00, 101.24it/s]\n",
      "100%|██████████| 236/236 [00:03<00:00, 76.88it/s] \n",
      "100%|██████████| 236/236 [00:02<00:00, 111.34it/s]\n",
      "100%|██████████| 236/236 [00:02<00:00, 112.87it/s]\n",
      "100%|██████████| 179/179 [00:03<00:00, 56.78it/s] \n",
      "100%|██████████| 179/179 [00:03<00:00, 45.17it/s] \n",
      "100%|██████████| 179/179 [00:02<00:00, 61.58it/s] \n",
      "100%|██████████| 179/179 [00:03<00:00, 46.61it/s] \n",
      "100%|██████████| 411/411 [00:05<00:00, 77.19it/s] \n",
      "100%|██████████| 411/411 [00:05<00:00, 74.96it/s] \n",
      "100%|██████████| 411/411 [00:04<00:00, 87.89it/s] \n",
      "100%|██████████| 411/411 [00:05<00:00, 70.49it/s] \n",
      "100%|██████████| 203/203 [00:00<00:00, 741.14it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 795.02it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 796.99it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 803.51it/s]\n",
      "100%|██████████| 832/832 [00:01<00:00, 697.69it/s]\n",
      "100%|██████████| 832/832 [00:01<00:00, 811.26it/s]\n",
      "100%|██████████| 832/832 [00:01<00:00, 767.32it/s]\n",
      "100%|██████████| 832/832 [00:01<00:00, 818.78it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1515.98it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1531.84it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1616.76it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1664.47it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 753.89it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 752.63it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 769.50it/s]\n",
      "100%|██████████| 2167/2167 [00:03<00:00, 560.91it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 768.03it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 869.66it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 763.24it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 802.75it/s]\n",
      "100%|██████████| 305/305 [00:08<00:00, 35.38it/s]\n",
      "100%|██████████| 305/305 [00:05<00:00, 53.20it/s]\n",
      "100%|██████████| 305/305 [00:06<00:00, 50.02it/s]\n",
      "100%|██████████| 305/305 [00:07<00:00, 40.64it/s]\n",
      "100%|██████████| 254/254 [00:06<00:00, 41.07it/s]\n",
      "100%|██████████| 254/254 [00:04<00:00, 50.80it/s]\n",
      "100%|██████████| 254/254 [00:06<00:00, 38.15it/s]\n",
      "100%|██████████| 254/254 [00:05<00:00, 47.65it/s]\n",
      "100%|██████████| 261/261 [00:02<00:00, 101.69it/s]\n",
      "100%|██████████| 261/261 [00:02<00:00, 120.87it/s]\n",
      "100%|██████████| 261/261 [00:03<00:00, 74.81it/s] \n",
      "100%|██████████| 261/261 [00:02<00:00, 118.57it/s]\n",
      "100%|██████████| 238/238 [00:02<00:00, 89.90it/s] \n",
      "100%|██████████| 238/238 [00:01<00:00, 122.29it/s]\n",
      "100%|██████████| 238/238 [00:02<00:00, 109.22it/s]\n",
      "100%|██████████| 238/238 [00:02<00:00, 107.62it/s]\n",
      "100%|██████████| 909/909 [00:39<00:00, 23.27it/s] \n",
      "100%|██████████| 909/909 [00:31<00:00, 29.05it/s] \n",
      "100%|██████████| 909/909 [01:17<00:00, 11.72it/s] \n",
      "100%|██████████| 909/909 [01:30<00:00,  9.99it/s]\n",
      "100%|██████████| 338/338 [00:01<00:00, 243.05it/s]\n",
      "100%|██████████| 338/338 [00:07<00:00, 45.93it/s] \n",
      "100%|██████████| 338/338 [00:01<00:00, 232.20it/s]\n",
      "100%|██████████| 338/338 [00:01<00:00, 182.34it/s]\n",
      "100%|██████████| 342/342 [00:08<00:00, 40.14it/s] \n",
      "100%|██████████| 342/342 [00:02<00:00, 168.58it/s]\n",
      "100%|██████████| 342/342 [00:02<00:00, 156.25it/s]\n",
      "100%|██████████| 342/342 [00:01<00:00, 172.05it/s]\n",
      "100%|██████████| 309/309 [00:01<00:00, 273.84it/s]\n",
      "100%|██████████| 309/309 [00:01<00:00, 258.27it/s]\n",
      "100%|██████████| 309/309 [00:01<00:00, 262.49it/s]\n",
      "100%|██████████| 309/309 [00:01<00:00, 283.67it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 2194.24it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 2571.35it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 2802.04it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 2825.30it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 2755.39it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 2659.68it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 2757.42it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 2738.24it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3143.43it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3216.84it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3140.94it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3180.41it/s]\n",
      "100%|██████████| 21240/21240 [00:09<00:00, 2231.46it/s]\n",
      "100%|██████████| 21240/21240 [00:03<00:00, 6244.95it/s]\n",
      "100%|██████████| 21240/21240 [00:03<00:00, 5885.20it/s]\n",
      "100%|██████████| 21240/21240 [00:03<00:00, 6471.45it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 6449.00it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 6455.79it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 6415.53it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 6490.72it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2140.71it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 1890.73it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2260.58it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2188.55it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert all examples to a GPT-3 style input string.\n",
    "\n",
    "(dataset, split, example_id, local_context, include_speaker, input, expected_output)\n",
    "\n",
    "Scored using uncased exact match.\n",
    "\n",
    "Prompt:\n",
    "```\n",
    "Final Exam with Answer Key\n",
    "Instructions: Please carefully read the following passages. For each\n",
    "passage, you must identify which noun the pronoun marked in *bold* refers\n",
    "to.\n",
    "=====\n",
    "Passage: Mr. Moncrieff visited Chester’s luxurious New York apartment,\n",
    "thinking that it belonged to his son Edward. The result was that Mr.\n",
    "Moncrieff has decided to cancel Edward’s allowance on the ground that\n",
    "he no longer requires *his* financial support.\n",
    "Question: In the passage above, what does the pronoun \"*his*\" refer to?\n",
    "Answer:\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def get_all_examples(config_name, split, dataset):\n",
    "    examples = []\n",
    "    for use_local_context, include_speaker in itertools.product([True, False], [True, False]):\n",
    "        examples += get_examples(config_name, split, dataset,\n",
    "                        use_local_context=use_local_context, include_speaker=include_speaker)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def main():\n",
    "    examples = []\n",
    "    for config_name in config_names:\n",
    "        dataset_name = \"coref-data/pcr_single_antecedent\"\n",
    "        dataset = datasets.load_dataset(dataset_name, config_name)\n",
    "        for split in [\"validation\", \"test\"]:\n",
    "            if split not in dataset:\n",
    "                continue\n",
    "            examples += get_all_examples(config_name, split, dataset[split])\n",
    "    return examples\n",
    "\n",
    "data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152436"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Exam with Answer Key\n",
      "Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to.\n",
      "=====\n",
      "Passage: \n",
      "\n",
      "Speaker#1:\n",
      " The world 's fifth Disney park will soon open to the public here .\n",
      "\n",
      "Zhou_liangshuyi:\n",
      " The most important thing about Disney is that it is a global brand . Well , for several years , although *it* was still under construction and , er , not yet open , it can be said that many people have viewed Hong Kong with new respect .\n",
      "Question: In the passage above, what does the pronoun \"*it*\" refer to?\n",
      "Answer: \n",
      "********************\n",
      "the world 's fifth disney park\n"
     ]
    }
   ],
   "source": [
    "for d in data[:1]:\n",
    "    print(d[\"input\"])\n",
    "    print(\"*\"*20)\n",
    "    print(d[\"expected_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 153/153 [00:01<00:00, 130.89ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:16<00:00, 16.09s/it]\n",
      "README.md: 100%|██████████| 662/662 [00:00<00:00, 5.87MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/coref-data/pcr_gpt3_prompt/commit/d94cf844f81add2ae267f1c9008b461fc0d73b87', commit_message='Upload dataset', commit_description='', oid='d94cf844f81add2ae267f1c9008b461fc0d73b87', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_list(data)\n",
    "dataset.push_to_hub(\"coref-data/pcr_gpt3_prompt\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaphora_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
