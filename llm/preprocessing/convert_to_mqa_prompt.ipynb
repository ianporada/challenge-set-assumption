{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianporada/Research/envs/anaphora_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_names = [\n",
    "    'conll2012_indiscrim_english_v4',\n",
    "    'gum_indiscrim_ontogum',\n",
    "    'arrau_indiscrim_default',\n",
    "    'gap_indiscrim_default',\n",
    "    'davis_pdp_indiscrim_default',\n",
    "    'preco_indiscrim_default',\n",
    "    'litbank_indiscrim_split_0',\n",
    "    'gum_indiscrim_original',\n",
    "    'phrase_detectives_indiscrim_default',\n",
    "    'mmc_indiscrim_mmc_en',\n",
    "    'davis_wsc_indiscrim_wsc273',\n",
    "    'superglue_wsc_indiscrim_default',\n",
    "    'dpr_indiscrim_default',\n",
    "    'knowref_60k_indiscrim_default',\n",
    "    'pronominal_winogrande_default'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_mapping(sentences, context_start, context_end):\n",
    "    local_to_global = {}\n",
    "    global_to_local = {}\n",
    "    t = 0\n",
    "    for s_i in range(context_start, context_end):\n",
    "        sentence = sentences[s_i]\n",
    "        for i in range(len(sentence[\"tokens\"])):\n",
    "            local_to_global[(s_i, i)] = t\n",
    "            global_to_local[t] = (s_i, i)\n",
    "            t += 1\n",
    "    return local_to_global, global_to_local\n",
    "\n",
    "\n",
    "def local_mention_to_global(local_to_global, mention):\n",
    "    sent, start, end = mention\n",
    "    return (\n",
    "                local_to_global[(sent, start)],\n",
    "                local_to_global[(sent, end)]\n",
    "            )\n",
    "\n",
    "\n",
    "def global_mention_to_local(global_to_local, mention):\n",
    "    start, end = mention\n",
    "    start_sent, start_tok = global_to_local[start]\n",
    "    end_sent, end_tok = global_to_local[end]\n",
    "    assert start_sent == end_sent and end_tok >= start_tok\n",
    "    return [start_sent, start_tok, end_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(config_name, split, dataset, use_local_context, include_speaker):\n",
    "    examples = []\n",
    "    for ex in tqdm(dataset):\n",
    "        sentences = ex[\"sentences\"]\n",
    "\n",
    "        context_start = 0\n",
    "        context_end = len(sentences)\n",
    "\n",
    "        ex_id = ex[\"id\"]\n",
    "        psent, pstart, pend = ex[\"pronoun\"]\n",
    "        ex_id = str(ex[\"id\"]) + f\"_{psent}_{pstart}_{pend}\"\n",
    "        \n",
    "        if use_local_context:\n",
    "            context_start = ex[\"local_context_start\"]\n",
    "            context_end = ex[\"local_context_end\"]\n",
    "\n",
    "        local_to_global, global_to_local = get_token_mapping(sentences, context_start, context_end)\n",
    "        words = [[x[\"text\"] for x in s[\"tokens\"]] for s in sentences[context_start:context_end]]\n",
    "\n",
    "        speakers = None\n",
    "        if include_speaker:\n",
    "            speakers = [[s[\"speaker\"] if s[\"speaker\"] is not None else \"\"]*len(s[\"tokens\"])\n",
    "                        for s in sentences[context_start:context_end]]\n",
    "            speakers = [spk for s in speakers for spk in s]\n",
    "\n",
    "        lm_to_global = partial(local_mention_to_global, local_to_global)\n",
    "        mentions = [lm_to_global(ex[\"pronoun\"]),\n",
    "                    lm_to_global(ex[\"antecedents\"][0]),\n",
    "                    lm_to_global(ex[\"distractors\"][0])] # (start, end)\n",
    "        \n",
    "        # make sure each\n",
    "        instructions = \"Instruction: Please carefully read the following passages. \" \\\n",
    "                \"For each passage, you must identify which noun the mention \" \\\n",
    "                \"marked in *bold* refers to. \" \\\n",
    "                \"If the marked mention does not have any antecedent, \" \\\n",
    "                \"please select “no antecedent”.\\n\\n\"\n",
    "        \n",
    "        passage_words = [w for s in words for w in s]\n",
    "\n",
    "        global_antecedent = mentions[1]\n",
    "        expected_output_words = passage_words[global_antecedent[0] : global_antecedent[1] + 1]\n",
    "        expected_output = \" \".join(expected_output_words).lower()\n",
    "\n",
    "        global_distractor = mentions[2]\n",
    "        global_distractor_words = passage_words[global_distractor[0] : global_distractor[1] + 1]\n",
    "        negative_output = \" \".join(global_distractor_words).lower()\n",
    "\n",
    "        global_pronoun = mentions[0]\n",
    "        assert global_pronoun[0] == global_pronoun[1], \"Pronoun should be exactly one word\"\n",
    "        original_pronoun = passage_words[global_pronoun[0]]\n",
    "        passage_words[global_pronoun[0]] = f\"*{original_pronoun}*\" # add astericks around pronoun\n",
    "\n",
    "        # add square brackets to words in passage\n",
    "        passage_words[global_antecedent[0]] = f\"[{passage_words[global_antecedent[0]]}\"\n",
    "        passage_words[global_antecedent[1]] = f\"{passage_words[global_antecedent[1]]}]\"\n",
    "\n",
    "        passage_words[global_distractor[0]] = f\"[{passage_words[global_distractor[0]]}\"\n",
    "        passage_words[global_distractor[1]] = f\"{passage_words[global_distractor[1]]}]\"\n",
    "\n",
    "        if include_speaker:\n",
    "            last_speaker = None\n",
    "            passage = \"\"\n",
    "            for i, w in enumerate(passage_words):\n",
    "                curr_speaker = speakers[i] if speakers[i] else \"Anonymous\"\n",
    "                if curr_speaker != last_speaker:\n",
    "                    passage += f\"\\n\\n{curr_speaker}:\\n\"\n",
    "                    last_speaker = curr_speaker\n",
    "                passage += (\" \" if passage else \"\") + w\n",
    "        else:\n",
    "            passage = \" \".join(passage_words)\n",
    "\n",
    "        candidates = [\"no antecedent\", expected_output, negative_output]\n",
    "        answer_order = list(range(3))\n",
    "        random.shuffle(answer_order)\n",
    "        ordered_candidates = [candidates[i] for i in answer_order]\n",
    "        candidate_to_order = [answer_order.index(i) for i in range(3)]\n",
    "\n",
    "        choices = f\"A. {ordered_candidates[0]}\\n\" + \\\n",
    "                  f\"B. {ordered_candidates[1]}\\n\" + \\\n",
    "                  f\"C. {ordered_candidates[2]}\"\n",
    "\n",
    "        question = f\"What does \\\"*{original_pronoun}*\\\" refer to?\"\n",
    "\n",
    "        input_str = instructions + \"Context: \" + passage + \"\\n\" + \\\n",
    "            \"Choices:\\n\" + choices + \"\\n\" + \\\n",
    "            \"Question: \" + question + \"\\n\" + \\\n",
    "            \"Answer: \"\n",
    "        \n",
    "        letters = [\"A\", \"B\", \"C\"]\n",
    "        expected_output = letters[candidate_to_order[1]]\n",
    "        negative_output = letters[candidate_to_order[2]]\n",
    "        \n",
    "        # (dataset, split, example_id, local_context, include_speaker, input, expected_output)\n",
    "        output_example = {\n",
    "            \"dataset\": config_name,\n",
    "            \"split\": split,\n",
    "            \"example_id\": ex_id,\n",
    "            \"local_context\": use_local_context,\n",
    "            \"include_speaker\": include_speaker,\n",
    "            \"input\": input_str,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"negative_output\": negative_output,\n",
    "            \"passage_words\": passage_words,\n",
    "            \"mentions\": mentions,\n",
    "        }\n",
    "        examples.append(output_example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [00:06<00:00, 220.14it/s]\n",
      "100%|██████████| 1536/1536 [00:06<00:00, 220.18it/s]\n",
      "100%|██████████| 1536/1536 [00:07<00:00, 199.03it/s]\n",
      "100%|██████████| 1536/1536 [00:07<00:00, 209.75it/s]\n",
      "100%|██████████| 1642/1642 [00:07<00:00, 219.04it/s]\n",
      "100%|██████████| 1642/1642 [00:07<00:00, 222.23it/s]\n",
      "100%|██████████| 1642/1642 [00:07<00:00, 215.99it/s]\n",
      "100%|██████████| 1642/1642 [00:07<00:00, 210.56it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 117.60it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 106.40it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 100.46it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 102.82it/s]\n",
      "100%|██████████| 236/236 [00:02<00:00, 112.90it/s]\n",
      "100%|██████████| 236/236 [00:02<00:00, 115.02it/s]\n",
      "100%|██████████| 236/236 [00:02<00:00, 108.14it/s]\n",
      "100%|██████████| 236/236 [00:02<00:00, 107.31it/s]\n",
      "100%|██████████| 179/179 [00:03<00:00, 58.66it/s] \n",
      "100%|██████████| 179/179 [00:03<00:00, 57.27it/s] \n",
      "100%|██████████| 179/179 [00:03<00:00, 53.56it/s] \n",
      "100%|██████████| 179/179 [00:03<00:00, 56.50it/s] \n",
      "100%|██████████| 411/411 [00:04<00:00, 84.61it/s] \n",
      "100%|██████████| 411/411 [00:04<00:00, 88.26it/s] \n",
      "100%|██████████| 411/411 [00:05<00:00, 75.07it/s] \n",
      "100%|██████████| 411/411 [00:05<00:00, 77.43it/s] \n",
      "100%|██████████| 203/203 [00:00<00:00, 642.57it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 725.26it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 766.70it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 743.54it/s]\n",
      "100%|██████████| 832/832 [00:01<00:00, 782.34it/s]\n",
      "100%|██████████| 832/832 [00:01<00:00, 780.66it/s]\n",
      "100%|██████████| 832/832 [00:01<00:00, 808.40it/s]\n",
      "100%|██████████| 832/832 [00:01<00:00, 790.57it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1625.37it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1667.53it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1690.03it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1670.13it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 795.65it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 800.01it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 740.57it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 797.36it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 788.90it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 819.95it/s]\n",
      "100%|██████████| 2248/2248 [00:03<00:00, 708.89it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 771.01it/s]\n",
      "100%|██████████| 305/305 [00:06<00:00, 50.14it/s]\n",
      "100%|██████████| 305/305 [00:06<00:00, 47.93it/s]\n",
      "100%|██████████| 305/305 [00:06<00:00, 44.21it/s]\n",
      "100%|██████████| 305/305 [00:06<00:00, 45.37it/s]\n",
      "100%|██████████| 254/254 [00:05<00:00, 49.48it/s]\n",
      "100%|██████████| 254/254 [00:05<00:00, 44.85it/s]\n",
      "100%|██████████| 254/254 [00:05<00:00, 43.14it/s]\n",
      "100%|██████████| 254/254 [00:06<00:00, 39.65it/s]\n",
      "100%|██████████| 261/261 [00:02<00:00, 110.10it/s]\n",
      "100%|██████████| 261/261 [00:02<00:00, 91.72it/s] \n",
      "100%|██████████| 261/261 [00:02<00:00, 99.20it/s] \n",
      "100%|██████████| 261/261 [00:03<00:00, 81.31it/s] \n",
      "100%|██████████| 238/238 [00:02<00:00, 100.53it/s]\n",
      "100%|██████████| 238/238 [00:02<00:00, 115.39it/s]\n",
      "100%|██████████| 238/238 [00:03<00:00, 76.44it/s] \n",
      "100%|██████████| 238/238 [00:02<00:00, 110.51it/s]\n",
      "100%|██████████| 909/909 [00:26<00:00, 34.78it/s] \n",
      "100%|██████████| 909/909 [00:29<00:00, 30.36it/s] \n",
      "100%|██████████| 909/909 [00:51<00:00, 17.81it/s]\n",
      "100%|██████████| 909/909 [01:22<00:00, 10.96it/s]\n",
      "100%|██████████| 338/338 [00:04<00:00, 83.25it/s] \n",
      "100%|██████████| 338/338 [00:02<00:00, 136.06it/s]\n",
      "100%|██████████| 338/338 [00:04<00:00, 79.45it/s] \n",
      "100%|██████████| 338/338 [00:05<00:00, 65.27it/s] \n",
      "100%|██████████| 342/342 [00:02<00:00, 148.93it/s]\n",
      "100%|██████████| 342/342 [00:04<00:00, 70.43it/s] \n",
      "100%|██████████| 342/342 [00:02<00:00, 165.94it/s]\n",
      "100%|██████████| 342/342 [00:05<00:00, 60.25it/s] \n",
      "100%|██████████| 309/309 [00:01<00:00, 243.51it/s]\n",
      "100%|██████████| 309/309 [00:01<00:00, 256.28it/s]\n",
      "100%|██████████| 309/309 [00:01<00:00, 270.77it/s]\n",
      "100%|██████████| 309/309 [00:01<00:00, 288.05it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 2264.95it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 2616.23it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 2768.59it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 2814.92it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 2144.91it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 1854.53it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 2004.70it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 1912.92it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3015.32it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3004.38it/s]\n",
      "100%|██████████| 558/558 [00:01<00:00, 434.59it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3101.45it/s]\n",
      "100%|██████████| 21240/21240 [00:03<00:00, 5692.36it/s]\n",
      "100%|██████████| 21240/21240 [00:04<00:00, 4836.64it/s]\n",
      "100%|██████████| 21240/21240 [00:06<00:00, 3140.74it/s]\n",
      "100%|██████████| 21240/21240 [00:03<00:00, 6336.95it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 5814.94it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 5843.79it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 6137.21it/s]\n",
      "100%|██████████| 3061/3061 [00:01<00:00, 1581.00it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2326.69it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2391.61it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2375.62it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2547.51it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert all examples to a GPT-3 style input string.\n",
    "\n",
    "(dataset, split, example_id, local_context, include_speaker, input, expected_output)\n",
    "\n",
    "Scored using uncased exact match.\n",
    "\"\"\"\n",
    "\n",
    "def get_all_examples(config_name, split, dataset):\n",
    "    examples = []\n",
    "    for use_local_context, include_speaker in itertools.product([True, False], [True, False]):\n",
    "        examples += get_examples(config_name, split, dataset,\n",
    "                        use_local_context=use_local_context, include_speaker=include_speaker)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def main():\n",
    "    examples = []\n",
    "    for config_name in config_names:\n",
    "        dataset_name = \"coref-data/pcr_single_antecedent\"\n",
    "        dataset = datasets.load_dataset(dataset_name, config_name)\n",
    "        for split in [\"validation\", \"test\"]:\n",
    "            if split not in dataset:\n",
    "                continue\n",
    "            examples += get_all_examples(config_name, split, dataset[split])\n",
    "    return examples\n",
    "\n",
    "data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152436"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Please carefully read the following passages. For each passage, you must identify which noun the mention marked in *bold* refers to. If the marked mention does not have any antecedent, please select “no antecedent”.\n",
      "\n",
      "Context: \n",
      "\n",
      "Speaker#1:\n",
      " [The world 's fifth [Disney] park] will soon open to the public here .\n",
      "\n",
      "Zhou_liangshuyi:\n",
      " The most important thing about Disney is that it is a global brand . Well , for several years , although *it* was still under construction and , er , not yet open , it can be said that many people have viewed Hong Kong with new respect .\n",
      "Choices:\n",
      "A. the world 's fifth disney park\n",
      "B. disney\n",
      "C. no antecedent\n",
      "Question: What does \"*it*\" refer to?\n",
      "Answer: \n",
      "********************\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "for d in data[:1]:\n",
    "    print(d[\"input\"])\n",
    "    print(\"*\"*20)\n",
    "    print(d[\"expected_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 153/153 [00:01<00:00, 121.17ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:11<00:00, 11.92s/it]\n",
      "README.md: 100%|██████████| 662/662 [00:00<00:00, 4.48MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/coref-data/pcr_mqa_prompt/commit/5b60313af5a6f5a43dd7f76f5039b1939eeb5f20', commit_message='Upload dataset', commit_description='', oid='5b60313af5a6f5a43dd7f76f5039b1939eeb5f20', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_list(data)\n",
    "dataset.push_to_hub(\"coref-data/pcr_mqa_prompt\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaphora_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
