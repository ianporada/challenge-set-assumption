{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_names = [\n",
    "    'conll2012_indiscrim_english_v4',\n",
    "    'gum_indiscrim_ontogum',\n",
    "    'arrau_indiscrim_default',\n",
    "    'gap_indiscrim_default',\n",
    "    'davis_pdp_indiscrim_default',\n",
    "    'preco_indiscrim_default',\n",
    "    'litbank_indiscrim_split_0',\n",
    "    'gum_indiscrim_original',\n",
    "    'phrase_detectives_indiscrim_default',\n",
    "    'mmc_indiscrim_mmc_en',\n",
    "    'davis_wsc_indiscrim_wsc273',\n",
    "    'superglue_wsc_indiscrim_default',\n",
    "    'dpr_indiscrim_default',\n",
    "    'knowref_60k_indiscrim_default',\n",
    "    'pronominal_winogrande_default'\n",
    "]\n",
    "\n",
    "# config_names = [\n",
    "#     'davis_wsc_indiscrim_wsc273',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_mapping(sentences, context_start, context_end):\n",
    "    local_to_global = {}\n",
    "    global_to_local = {}\n",
    "    t = 0\n",
    "    for s_i in range(context_start, context_end):\n",
    "        sentence = sentences[s_i]\n",
    "        for i in range(len(sentence[\"tokens\"])):\n",
    "            local_to_global[(s_i, i)] = t\n",
    "            global_to_local[t] = (s_i, i)\n",
    "            t += 1\n",
    "    return local_to_global, global_to_local\n",
    "\n",
    "\n",
    "def local_mention_to_global(local_to_global, mention):\n",
    "    sent, start, end = mention\n",
    "    return (\n",
    "                local_to_global[(sent, start)],\n",
    "                local_to_global[(sent, end)]\n",
    "            )\n",
    "\n",
    "\n",
    "def global_mention_to_local(global_to_local, mention):\n",
    "    start, end = mention\n",
    "    start_sent, start_tok = global_to_local[start]\n",
    "    end_sent, end_tok = global_to_local[end]\n",
    "    assert start_sent == end_sent and end_tok >= start_tok\n",
    "    return [start_sent, start_tok, end_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(config_name, split, dataset, use_local_context, include_speaker):\n",
    "    examples = []\n",
    "    for ex in tqdm(dataset):\n",
    "        sentences = ex[\"sentences\"]\n",
    "\n",
    "        context_start = 0\n",
    "        context_end = len(sentences)\n",
    "\n",
    "        ex_id = ex[\"id\"]\n",
    "        psent, pstart, pend = ex[\"pronoun\"]\n",
    "        ex_id = str(ex[\"id\"]) + f\"_{psent}_{pstart}_{pend}\"\n",
    "        \n",
    "        if use_local_context:\n",
    "            context_start = ex[\"local_context_start\"]\n",
    "            context_end = ex[\"local_context_end\"]\n",
    "\n",
    "        local_to_global, global_to_local = get_token_mapping(sentences, context_start, context_end)\n",
    "        words = [[x[\"text\"] for x in s[\"tokens\"]] for s in sentences[context_start:context_end]]\n",
    "\n",
    "        speakers = None\n",
    "        if include_speaker:\n",
    "            speakers = [[s[\"speaker\"] if s[\"speaker\"] is not None else \"\"]*len(s[\"tokens\"])\n",
    "                        for s in sentences[context_start:context_end]]\n",
    "            speakers = [spk for s in speakers for spk in s]\n",
    "\n",
    "        lm_to_global = partial(local_mention_to_global, local_to_global)\n",
    "        mentions = [lm_to_global(ex[\"pronoun\"]),\n",
    "                    lm_to_global(ex[\"antecedents\"][0]),\n",
    "                    lm_to_global(ex[\"distractors\"][0])] # (start, end)\n",
    "        \n",
    "        # make sure each\n",
    "        instructions = \"Annotate all entity mentions in the following text with coreference clusters. \" \\\n",
    "                \"Use Markdown tags to indicate clusters in the output, \" \\\n",
    "                \"with the following format [mention](#cluster_name)\\n\\n\"\n",
    "        \n",
    "        passage_words = [w for s in words for w in s]\n",
    "\n",
    "        global_antecedent = mentions[1]\n",
    "        expected_output_words = passage_words[global_antecedent[0] : global_antecedent[1] + 1]\n",
    "        expected_output = \" \".join(expected_output_words).lower()\n",
    "\n",
    "        global_distractor = mentions[2]\n",
    "        global_distractor_words = passage_words[global_distractor[0] : global_distractor[1] + 1]\n",
    "        negative_output = \" \".join(global_distractor_words).lower()\n",
    "\n",
    "        global_pronoun = mentions[0]\n",
    "        assert global_pronoun[0] == global_pronoun[1], \"Pronoun should be exactly one word\"\n",
    "        original_pronoun = passage_words[global_pronoun[0]]\n",
    "        passage_words[global_pronoun[0]] = f\"[{original_pronoun}](#)\" # add astericks around pronoun\n",
    "\n",
    "        cluster_ids = [0, 1]\n",
    "        random.shuffle(cluster_ids)\n",
    "        gold_id = cluster_ids[0]\n",
    "        distractor_id = cluster_ids[1]\n",
    "\n",
    "        # add square brackets to words in passage\n",
    "        passage_words[global_antecedent[0]] = f\"[{passage_words[global_antecedent[0]]}\"\n",
    "        passage_words[global_antecedent[1]] = f\"{passage_words[global_antecedent[1]]}](cluster_{gold_id})\"\n",
    "\n",
    "        passage_words[global_distractor[0]] = f\"[{passage_words[global_distractor[0]]}\"\n",
    "        passage_words[global_distractor[1]] = f\"{passage_words[global_distractor[1]]}](cluster_{distractor_id})\"\n",
    "\n",
    "        def words_to_passage(passage_words):\n",
    "            if include_speaker:\n",
    "                last_speaker = None\n",
    "                passage = \"\"\n",
    "                for i, w in enumerate(passage_words):\n",
    "                    curr_speaker = speakers[i] if speakers[i] else \"Anonymous\"\n",
    "                    if curr_speaker != last_speaker:\n",
    "                        passage += f\"\\n\\n{curr_speaker}:\\n\"\n",
    "                        last_speaker = curr_speaker\n",
    "                    passage += (\" \" if passage else \"\") + w\n",
    "            else:\n",
    "                passage = \" \".join(passage_words)\n",
    "            return passage\n",
    "        \n",
    "        passage = words_to_passage(passage_words)\n",
    "\n",
    "        question = f\"In the above passage, what \\\"*{original_pronoun}*\\\" refer to?\"\n",
    "\n",
    "        input_str = instructions + \"Input: \" + passage + \"\\n\" + \\\n",
    "            \"Output: \"\n",
    "        \n",
    "        passage_words[global_pronoun[0]] = f\"[{original_pronoun}](#cluster_{gold_id})\"\n",
    "        expected_output = words_to_passage(passage_words)\n",
    "\n",
    "        passage_words[global_pronoun[0]] = f\"[{original_pronoun}](#cluster_{distractor_id})\"\n",
    "        negative_output = words_to_passage(passage_words)\n",
    "        \n",
    "        # (dataset, split, example_id, local_context, include_speaker, input, expected_output)\n",
    "        output_example = {\n",
    "            \"dataset\": config_name,\n",
    "            \"split\": split,\n",
    "            \"example_id\": ex_id,\n",
    "            \"local_context\": use_local_context,\n",
    "            \"include_speaker\": include_speaker,\n",
    "            \"input\": input_str,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"negative_output\": negative_output,\n",
    "            \"passage_words\": passage_words,\n",
    "            \"mentions\": mentions,\n",
    "        }\n",
    "        examples.append(output_example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1536/1536 [00:06<00:00, 243.92it/s]\n",
      "100%|██████████| 1536/1536 [00:06<00:00, 245.20it/s]\n",
      "100%|██████████| 1536/1536 [00:07<00:00, 203.72it/s]\n",
      "100%|██████████| 1536/1536 [00:06<00:00, 231.62it/s]\n",
      "100%|██████████| 1642/1642 [00:07<00:00, 234.04it/s]\n",
      "100%|██████████| 1642/1642 [00:06<00:00, 252.89it/s]\n",
      "100%|██████████| 1642/1642 [00:07<00:00, 233.41it/s]\n",
      "100%|██████████| 1642/1642 [00:07<00:00, 231.65it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 114.35it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 130.73it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 118.46it/s]\n",
      "100%|██████████| 272/272 [00:02<00:00, 122.57it/s]\n",
      "100%|██████████| 236/236 [00:02<00:00, 112.57it/s]\n",
      "100%|██████████| 236/236 [00:01<00:00, 130.33it/s]\n",
      "100%|██████████| 236/236 [00:01<00:00, 121.29it/s]\n",
      "100%|██████████| 236/236 [00:01<00:00, 128.89it/s]\n",
      "100%|██████████| 179/179 [00:03<00:00, 55.69it/s] \n",
      "100%|██████████| 179/179 [00:02<00:00, 68.71it/s] \n",
      "100%|██████████| 179/179 [00:03<00:00, 54.10it/s] \n",
      "100%|██████████| 179/179 [00:02<00:00, 64.19it/s] \n",
      "100%|██████████| 411/411 [00:05<00:00, 80.24it/s] \n",
      "100%|██████████| 411/411 [00:04<00:00, 98.26it/s] \n",
      "100%|██████████| 411/411 [00:04<00:00, 82.31it/s] \n",
      "100%|██████████| 411/411 [00:04<00:00, 89.73it/s] \n",
      "100%|██████████| 203/203 [00:00<00:00, 806.91it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 924.03it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 903.20it/s]\n",
      "100%|██████████| 203/203 [00:00<00:00, 910.44it/s]\n",
      "100%|██████████| 832/832 [00:01<00:00, 831.14it/s]\n",
      "100%|██████████| 832/832 [00:00<00:00, 909.60it/s]\n",
      "100%|██████████| 832/832 [00:00<00:00, 902.03it/s]\n",
      "100%|██████████| 832/832 [00:00<00:00, 912.56it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 962.49it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1527.59it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1803.04it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 1882.36it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 825.78it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 865.70it/s]\n",
      "100%|██████████| 2167/2167 [00:03<00:00, 653.59it/s]\n",
      "100%|██████████| 2167/2167 [00:02<00:00, 807.97it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 779.52it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 859.16it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 772.18it/s]\n",
      "100%|██████████| 2248/2248 [00:02<00:00, 847.33it/s]\n",
      "100%|██████████| 305/305 [00:06<00:00, 44.15it/s]\n",
      "100%|██████████| 305/305 [00:05<00:00, 56.29it/s]\n",
      "100%|██████████| 305/305 [00:06<00:00, 48.30it/s]\n",
      "100%|██████████| 305/305 [00:05<00:00, 54.81it/s]\n",
      "100%|██████████| 254/254 [00:05<00:00, 50.16it/s]\n",
      "100%|██████████| 254/254 [00:05<00:00, 48.79it/s]\n",
      "100%|██████████| 254/254 [00:05<00:00, 49.04it/s]\n",
      "100%|██████████| 254/254 [00:04<00:00, 52.03it/s]\n",
      "100%|██████████| 261/261 [00:02<00:00, 94.81it/s] \n",
      "100%|██████████| 261/261 [00:01<00:00, 133.22it/s]\n",
      "100%|██████████| 261/261 [00:02<00:00, 125.10it/s]\n",
      "100%|██████████| 261/261 [00:01<00:00, 133.21it/s]\n",
      "100%|██████████| 238/238 [00:01<00:00, 130.24it/s]\n",
      "100%|██████████| 238/238 [00:01<00:00, 137.49it/s]\n",
      "100%|██████████| 238/238 [00:01<00:00, 130.07it/s]\n",
      "100%|██████████| 238/238 [00:02<00:00, 103.74it/s]\n",
      "100%|██████████| 909/909 [00:21<00:00, 42.66it/s] \n",
      "100%|██████████| 909/909 [00:20<00:00, 44.69it/s] \n",
      "100%|██████████| 909/909 [00:24<00:00, 37.47it/s] \n",
      "100%|██████████| 909/909 [00:24<00:00, 37.82it/s] \n",
      "100%|██████████| 338/338 [00:01<00:00, 174.11it/s]\n",
      "100%|██████████| 338/338 [00:01<00:00, 301.47it/s]\n",
      "100%|██████████| 338/338 [00:01<00:00, 174.14it/s]\n",
      "100%|██████████| 338/338 [00:01<00:00, 280.58it/s]\n",
      "100%|██████████| 342/342 [00:01<00:00, 173.64it/s]\n",
      "100%|██████████| 342/342 [00:01<00:00, 200.28it/s]\n",
      "100%|██████████| 342/342 [00:01<00:00, 189.24it/s]\n",
      "100%|██████████| 342/342 [00:02<00:00, 123.04it/s]\n",
      "100%|██████████| 309/309 [00:01<00:00, 303.20it/s]\n",
      "100%|██████████| 309/309 [00:00<00:00, 338.20it/s]\n",
      "100%|██████████| 309/309 [00:00<00:00, 328.77it/s]\n",
      "100%|██████████| 309/309 [00:00<00:00, 324.67it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 2254.79it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 3276.34it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 3184.04it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 3273.09it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 2172.39it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 3143.69it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 3155.04it/s]\n",
      "100%|██████████| 146/146 [00:00<00:00, 3169.71it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 2933.54it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3397.59it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3373.75it/s]\n",
      "100%|██████████| 558/558 [00:00<00:00, 3392.14it/s]\n",
      "100%|██████████| 21240/21240 [00:03<00:00, 6264.19it/s]\n",
      "100%|██████████| 21240/21240 [00:03<00:00, 5390.43it/s]\n",
      "100%|██████████| 21240/21240 [00:03<00:00, 6484.45it/s]\n",
      "100%|██████████| 21240/21240 [00:03<00:00, 6656.70it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 6457.69it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 6576.36it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 6603.67it/s]\n",
      "100%|██████████| 3061/3061 [00:00<00:00, 6189.53it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2196.92it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2762.65it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2723.83it/s]\n",
      "100%|██████████| 209/209 [00:00<00:00, 2770.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert all examples to a GPT-3 style input string.\n",
    "\n",
    "(dataset, split, example_id, local_context, include_speaker, input, expected_output)\n",
    "\n",
    "Scored using uncased exact match.\n",
    "\n",
    "Prompt:\n",
    "```\n",
    "Please carefully read the following passages. For each passage, you must identify\n",
    "which noun the mention marked in *bold* refers to.\n",
    "\n",
    "Passage: [Tom] and [Mary] go to [the park]. *It* was full of trees.\n",
    "Question: In the above passage, what does *It* refer to?\n",
    "Answer: *It* refers to [the park]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def get_all_examples(config_name, split, dataset):\n",
    "    examples = []\n",
    "    for use_local_context, include_speaker in itertools.product([True, False], [True, False]):\n",
    "        examples += get_examples(config_name, split, dataset,\n",
    "                        use_local_context=use_local_context, include_speaker=include_speaker)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def main():\n",
    "    examples = []\n",
    "    for config_name in config_names:\n",
    "        dataset_name = \"coref-data/pcr_single_antecedent\"\n",
    "        dataset = datasets.load_dataset(dataset_name, config_name)\n",
    "        for split in [\"validation\", \"test\"]:\n",
    "            if split not in dataset:\n",
    "                continue\n",
    "            examples += get_all_examples(config_name, split, dataset[split])\n",
    "    return examples\n",
    "\n",
    "data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152436"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotate all entity mentions in the following text with coreference clusters. Use Markdown tags to indicate clusters in the output, with the following format [mention](#cluster_name)\n",
      "\n",
      "Input: \n",
      "\n",
      "Speaker#1:\n",
      " [The world 's fifth [Disney](cluster_1) park](cluster_0) will soon open to the public here .\n",
      "\n",
      "Zhou_liangshuyi:\n",
      " The most important thing about Disney is that it is a global brand . Well , for several years , although [it](#) was still under construction and , er , not yet open , it can be said that many people have viewed Hong Kong with new respect .\n",
      "Output: \n",
      "********************\n",
      "\n",
      "\n",
      "Speaker#1:\n",
      " [The world 's fifth [Disney](cluster_1) park](cluster_0) will soon open to the public here .\n",
      "\n",
      "Zhou_liangshuyi:\n",
      " The most important thing about Disney is that it is a global brand . Well , for several years , although [it](#cluster_0) was still under construction and , er , not yet open , it can be said that many people have viewed Hong Kong with new respect .\n"
     ]
    }
   ],
   "source": [
    "for d in data[:1]:\n",
    "    print(d[\"input\"])\n",
    "    print(\"*\"*20)\n",
    "    print(d[\"expected_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 77/77 [00:01<00:00, 66.06ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 77/77 [00:00<00:00, 375.63ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:23<00:00, 11.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/coref-data/pcr_doc_prompt/commit/54f740a82a8a09b6bc3c1bfcedf9e5cfc45dae47', commit_message='Upload dataset', commit_description='', oid='54f740a82a8a09b6bc3c1bfcedf9e5cfc45dae47', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_list(data)\n",
    "dataset.push_to_hub(\"coref-data/pcr_doc_prompt\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaphora_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
