{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_personal_pronouns = [\"she\", \"her\", \"he\", \"him\", \"them\", \"they\", \"it\"]\n",
    "possessive_pronouns = [\"his\", \"hers\", \"its\", \"their\", \"theirs\"]\n",
    "demonstrative_pronouns = [\"this\", \"that\", \"these\", \"those\"]\n",
    "pronouns = third_personal_pronouns + possessive_pronouns + demonstrative_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coref-data/conll2012_indiscrim', 'english_v4'),\n",
       " ('coref-data/gum_indiscrim', 'ontogum'),\n",
       " 'coref-data/arrau_indiscrim',\n",
       " 'coref-data/gap_indiscrim',\n",
       " 'coref-data/davis_pdp_indiscrim',\n",
       " 'coref-data/preco_indiscrim',\n",
       " ('coref-data/litbank_indiscrim', 'split_0'),\n",
       " ('coref-data/gum_indiscrim', 'original'),\n",
       " 'coref-data/phrase_detectives_indiscrim',\n",
       " ('coref-data/mmc_indiscrim', 'mmc_en'),\n",
       " ('coref-data/davis_wsc_indiscrim', 'wsc273'),\n",
       " 'coref-data/superglue_wsc_indiscrim',\n",
       " 'coref-data/dpr_indiscrim',\n",
       " 'coref-data/knowref_60k_indiscrim',\n",
       " 'coref-data/pronominal_winogrande']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_natural = [\n",
    "    (\"coref-data/conll2012_indiscrim\", \"english_v4\"),\n",
    "    (\"coref-data/gum_indiscrim\", \"ontogum\"),\n",
    "    \"coref-data/arrau_indiscrim\",\n",
    "]\n",
    "\n",
    "additional_natural = [\n",
    "    \"coref-data/preco_indiscrim\",\n",
    "    (\"coref-data/litbank_indiscrim\", \"split_0\"),\n",
    "    (\"coref-data/gum_indiscrim\", \"original\"),\n",
    "    \"coref-data/phrase_detectives_indiscrim\",\n",
    "    (\"coref-data/mmc_indiscrim\", \"mmc_en\"),\n",
    "]\n",
    "\n",
    "natural_special = [\n",
    "    \"coref-data/gap_indiscrim\",\n",
    "    \"coref-data/davis_pdp_indiscrim\",\n",
    "]\n",
    "\n",
    "winograd_like = [\n",
    "    (\"coref-data/davis_wsc_indiscrim\", \"wsc273\"),\n",
    "    \"coref-data/superglue_wsc_indiscrim\",\n",
    "    \"coref-data/dpr_indiscrim\",\n",
    "    \"coref-data/knowref_60k_indiscrim\",\n",
    "    \"coref-data/pronominal_winogrande\"\n",
    "]\n",
    "\n",
    "dataset_names = original_natural + natural_special + additional_natural + winograd_like\n",
    "\n",
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Value, Sequence, Features\n",
    "\n",
    "dataset_features = Features({'sentences': [{'id': Value(dtype='float64', id=None),\n",
    "   'speaker': Value(dtype='string', id=None),\n",
    "   'text': Value(dtype='string', id=None),\n",
    "   'tokens': [{'deprel': Value(dtype='string', id=None),\n",
    "     'deps': Value(dtype='string', id=None),\n",
    "     'feats': Value(dtype='string', id=None),\n",
    "     'head': Value(dtype='int64', id=None),\n",
    "     'id': Value(dtype='int64', id=None),\n",
    "     'lemma': Value(dtype='string', id=None),\n",
    "     'misc': Value(dtype='string', id=None),\n",
    "     'text': Value(dtype='string', id=None),\n",
    "     'upos': Value(dtype='string', id=None),\n",
    "     'xpos': Value(dtype='string', id=None)}],\n",
    "   'misc': {'parse_tree': Value(dtype='string', id=None)}}],\n",
    " 'id': Value(dtype='string', id=None),\n",
    " 'text': Value(dtype='string', id=None),\n",
    " 'coref_chains': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\n",
    " 'genre': Value(dtype='string', id=None),\n",
    " 'meta_data': {'comment': Value(dtype='string', id=None)},\n",
    " 'cluster_index': Value(dtype='int64', id=None),\n",
    " 'pronoun': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    " 'antecedents': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
    " 'distractors': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
    " 'local_context_start': Value(dtype='int64', id=None),\n",
    " 'local_context_end': Value(dtype='int64', id=None),\n",
    " 'local_context': [{'id': Value(dtype='int64', id=None),\n",
    "   'misc': {'parse_tree': Value(dtype='string', id=None)},\n",
    "   'speaker': Value(dtype='string', id=None),\n",
    "   'text': Value(dtype='string', id=None),\n",
    "   'tokens': [{'deprel': Value(dtype='string', id=None),\n",
    "     'deps': Value(dtype='string', id=None),\n",
    "     'feats': Value(dtype='string', id=None),\n",
    "     'head': Value(dtype='int64', id=None),\n",
    "     'id': Value(dtype='float64', id=None),\n",
    "     'lemma': Value(dtype='string', id=None),\n",
    "     'misc': Value(dtype='string', id=None),\n",
    "     'text': Value(dtype='string', id=None),\n",
    "     'upos': Value(dtype='string', id=None),\n",
    "     'xpos': Value(dtype='string', id=None)}]}]})\n",
    "\n",
    "gum_features = Features({'sentences': [{'id': Value(dtype='int64', id=None),\n",
    "   'speaker': Value(dtype='string', id=None),\n",
    "   'text': Value(dtype='string', id=None),\n",
    "   'tokens': [{'deprel': Value(dtype='string', id=None),\n",
    "     'feats': Value(dtype='string', id=None),\n",
    "     'head': Value(dtype='int64', id=None),\n",
    "     'id': Value(dtype='float64', id=None),\n",
    "     'lemma': Value(dtype='string', id=None),\n",
    "     'misc': Value(dtype='string', id=None),\n",
    "     'text': Value(dtype='string', id=None),\n",
    "     'upos': Value(dtype='string', id=None),\n",
    "     'xpos': Value(dtype='string', id=None)}],\n",
    "   'misc': {'parse_tree': Value(dtype='string', id=None)}}],\n",
    " 'id': Value(dtype='string', id=None),\n",
    " 'text': Value(dtype='string', id=None),\n",
    " 'coref_chains': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\n",
    " 'genre': Value(dtype='string', id=None),\n",
    " 'meta_data': {'comment': Value(dtype='string', id=None)},\n",
    " 'cluster_index': Value(dtype='int64', id=None),\n",
    " 'pronoun': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
    " 'antecedents': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
    " 'distractors': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
    " 'local_context_start': Value(dtype='int64', id=None),\n",
    " 'local_context_end': Value(dtype='int64', id=None),\n",
    " 'local_context': [{'id': Value(dtype='int64', id=None),\n",
    "   'misc': {'parse_tree': Value(dtype='string', id=None)},\n",
    "   'speaker': Value(dtype='string', id=None),\n",
    "   'text': Value(dtype='string', id=None),\n",
    "   'tokens': [{'deprel': Value(dtype='string', id=None),\n",
    "     'feats': Value(dtype='string', id=None),\n",
    "     'head': Value(dtype='int64', id=None),\n",
    "     'id': Value(dtype='float64', id=None),\n",
    "     'lemma': Value(dtype='string', id=None),\n",
    "     'misc': Value(dtype='string', id=None),\n",
    "     'text': Value(dtype='string', id=None),\n",
    "     'upos': Value(dtype='string', id=None),\n",
    "     'xpos': Value(dtype='string', id=None)}]}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a single huggingface dataset with each config representing a source.\n",
    "Each instance contains:\n",
    "    1. Full text\n",
    "        1a. Local context\n",
    "    2. Pronoun location\n",
    "    3. Positive candidates\n",
    "    4. Negative candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_pronoun_mentions(sentences, coref_chains):\n",
    "    pments = []\n",
    "    for c_i, c in enumerate(coref_chains):\n",
    "        for m in c:\n",
    "            sent_i, start, end = m\n",
    "            if end - start != 0:\n",
    "                continue\n",
    "            if sentences[sent_i][\"tokens\"][start][\"text\"].lower() in pronouns:\n",
    "                pments.append({\"mention\": m, \"cluster_idx\": c_i})\n",
    "    return pments\n",
    "\n",
    "\n",
    "def find_neighbors(sentences, coref_chains, ment, cidx):\n",
    "    sent, start, end = ment\n",
    "\n",
    "    antecedents = []\n",
    "    distractors = []\n",
    "    for c_i, c in enumerate(coref_chains):\n",
    "        coreferring = c_i == cidx\n",
    "        for neighbor in c:\n",
    "            neighbor_sent, neighbor_start, neighbor_end = neighbor\n",
    "\n",
    "            if coreferring and neighbor == ment:\n",
    "                continue\n",
    "            \n",
    "            # only check preceding two sentences\n",
    "            if neighbor_sent < sent - 2 or neighbor_sent > sent:\n",
    "                continue\n",
    "               \n",
    "            # only check antecedent mentions\n",
    "            if neighbor_end >= start:\n",
    "                continue\n",
    "\n",
    "            # don't consider when neighbor is in pronouns\n",
    "            neighbor_len = neighbor_end - neighbor_start + 1\n",
    "            if neighbor_len == 1 and sentences[neighbor_sent][\"tokens\"][neighbor_start][\"text\"].lower() in pronouns:\n",
    "                continue\n",
    "\n",
    "            if coreferring:\n",
    "                antecedents.append(neighbor)\n",
    "            else:\n",
    "                distractors.append(neighbor)\n",
    "\n",
    "\n",
    "    return antecedents, distractors\n",
    "\n",
    "\n",
    "def example_to_instances(ex):\n",
    "    sentences = ex[\"sentences\"]\n",
    "    coref_chains = ex[\"coref_chains\"]\n",
    "\n",
    "    assert len(sentences) == 1 and len(coref_chains) == 1\n",
    "    sentences, coref_chains = sentences[0], coref_chains[0]\n",
    "\n",
    "    ex_dict = {k: v[0] for k, v in ex.items()}\n",
    "\n",
    "    # TODO: it might be interesting to check if the antecedent mention\n",
    "    # is a pronoun / verb / nominal mention etc.\n",
    "\n",
    "    pments = find_pronoun_mentions(sentences, coref_chains)\n",
    "    instances = []\n",
    "    for pment in pments:\n",
    "        ment = pment[\"mention\"]\n",
    "        sent, start, end = ment\n",
    "        cidx = pment[\"cluster_idx\"]\n",
    "        antecedents, distractors = find_neighbors(sentences, coref_chains, ment, cidx)\n",
    "        if len(antecedents) < 1 or len(distractors) < 1:\n",
    "            continue\n",
    "\n",
    "        local_context_start = max(0, sent - 2)\n",
    "        local_context_end = sent + 1\n",
    "        instance = {\n",
    "            \"cluster_index\": cidx,\n",
    "            \"pronoun\": ment,\n",
    "            \"antecedents\": antecedents,\n",
    "            \"distractors\": distractors,\n",
    "            \"local_context_start\": local_context_start,\n",
    "            \"local_context_end\": local_context_end,\n",
    "            \"local_context\": sentences[local_context_start:local_context_end],\n",
    "        }\n",
    "        instances.append(instance | ex_dict)\n",
    "\n",
    "    if not instances:\n",
    "        cols = [\"cluster_index\", \"pronoun\", \"antecedents\", \"distractors\", \"local_context\"]\n",
    "        cols += ex_dict.keys()\n",
    "        return {k: [] for k in cols}\n",
    "\n",
    "    instances_dict = {}\n",
    "    for k in instances[0].keys():\n",
    "        column = [x[k] for x in instances]\n",
    "        instances_dict[k] = column\n",
    "\n",
    "    return instances_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  coref-data/superglue_wsc_indiscrim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 0ba [00:00, ?ba/s]:00<?, ?it/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "Creating parquet from Arrow format: 0ba [00:00, ?ba/s]:00<?, ?it/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "Creating parquet from Arrow format: 0ba [00:00, ?ba/s]:00<?, ?it/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "README.md: 100%|██████████| 32.3k/32.3k [00:00<00:00, 40.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "new_dataset_name = \"coref-data/pronominal_coreference_resolution\"\n",
    "\n",
    "for dataset_name in dataset_names[11:12]:\n",
    "    if isinstance(dataset_name, tuple):\n",
    "        base_name, config_name = dataset_name\n",
    "    else:\n",
    "        base_name, config_name = dataset_name, \"default\"\n",
    "    \n",
    "    new_config_name = base_name.replace(\"coref-data/\", \"\") + \"_\" + config_name\n",
    "\n",
    "    print(\"Processing: \", dataset_name)\n",
    "\n",
    "    dataset = datasets.load_dataset(base_name, config_name)\n",
    "    # for ontogum we have to explicitly set features because Speaker is sometimes null\n",
    "    new_features = gum_features if \"gum\" in base_name else None\n",
    "    instances = dataset.map(example_to_instances, batched=True, batch_size=1,\n",
    "                            remove_columns=dataset[\"test\"].column_names, num_proc=1,\n",
    "                            features=new_features)\n",
    "    instances.push_to_hub(new_dataset_name, new_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  coref-data/superglue_wsc_indiscrim\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import datasets\n",
    "\n",
    "# for superglue, examples need to be merged\n",
    "\n",
    "new_dataset_name = \"coref-data/pronominal_coreference_resolution\"\n",
    "\n",
    "dataset_name = dataset_names[11]\n",
    "\n",
    "if isinstance(dataset_name, tuple):\n",
    "    base_name, config_name = dataset_name\n",
    "else:\n",
    "    base_name, config_name = dataset_name, \"default\"\n",
    "\n",
    "new_config_name = base_name.replace(\"coref-data/\", \"\") + \"_\" + config_name\n",
    "\n",
    "print(\"Processing: \", dataset_name)\n",
    "\n",
    "dataset = datasets.load_dataset(base_name, config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge examples with identical text\n",
    "new_examples = []\n",
    "\n",
    "for split, data in dataset.items():\n",
    "    text_to_examples = collections.defaultdict(list)\n",
    "    for ex in data:\n",
    "        text_to_examples[ex[\"text\"]].append(ex)\n",
    "\n",
    "    for text, examples in text_to_examples.items():\n",
    "        # multiple examples for the same text\n",
    "        if len(examples) != 2:\n",
    "            continue\n",
    "        ex_one_cluster, ex_two_clusters = examples[0], examples[1]\n",
    "        if len(ex_one_cluster[\"coref_chains\"]) == 2:\n",
    "            ex_one_cluster, ex_two_clusters = ex_two_clusters, ex_one_cluster\n",
    "        \n",
    "        # incorrect mention count\n",
    "        if len(ex_one_cluster[\"coref_chains\"]) != 1 or len(ex_two_clusters[\"coref_chains\"]) != 2:\n",
    "            continue\n",
    "        \n",
    "        existing_cluster = ex_one_cluster[\"coref_chains\"][0]\n",
    "        for cluster in ex_two_clusters[\"coref_chains\"]:\n",
    "            assert len(cluster) == 1\n",
    "            mention = cluster[0]\n",
    "            if mention not in existing_cluster:\n",
    "                ex_one_cluster[\"coref_chains\"].append(cluster)\n",
    "        assert len(ex_one_cluster[\"coref_chains\"]) == 2\n",
    "        new_examples.append(ex_one_cluster)\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    \"test\": datasets.Dataset.from_list(new_examples)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 196/196 [00:00<00:00, 803.80 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 42.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/coref-data/pronominal_coreference_resolution/commit/d19f8798982eec37fd4fdbb0d14b7faf54f35dde', commit_message='Upload dataset', commit_description='', oid='d19f8798982eec37fd4fdbb0d14b7faf54f35dde', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for ontogum we have to explicitly set features because Speaker is sometimes null\n",
    "new_features = gum_features if \"gum\" in base_name else None\n",
    "instances = dataset.map(example_to_instances, batched=True, batch_size=1,\n",
    "                        remove_columns=dataset[\"test\"].column_names, num_proc=1,\n",
    "                        features=new_features)\n",
    "instances.push_to_hub(new_dataset_name, new_config_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaphora_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
